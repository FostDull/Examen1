# -*- coding: utf-8 -*-
"""Villarreal_Jonathan-Examen1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qeGW1pkb1Vg4bz079d9OA16CEZLX_e5N

#UNIVERSIDAD CENTRAL DEL ECUADOR
###FACULTAD DE INGENIERÍA Y CIENCIAS APLICADAS
###NOMBRE: Villarreal Cardenas Jonathan Rolando
###MATERA: Desarrollo de Sistemas de Información
###FECHA: 19/05/2025
**EXÁMEN1**
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup  as bs

#Función que  convierte una lista de elementos en un dataframe de pandas
def rowsToDataFrame(rows):
  if not rows:
    return pd.DataFrame()
  header = rows[0]
  data = rows[1:]
  #Filtra las filas que tengan la misma longitud de filas que en el header
  num_cols = len(header)
  cleaned_data = [row for row in data if len(row) == num_cols]
  #Uso de columnas genéricas para filas con longitud diferente
  if any(len(row) != num_cols for row in data):
    header = [f"Col{i+1}" for i in range(max(len(row) for row in data))]
    cleaned_data = data
  return pd.DataFrame(cleaned_data, columns=header)

#Funcipon para extraer los datos de una tabla HTML
def processTableData(tbl):
  rows = []
  thead = tbl.find('thead')
  if thead:
    header = [th.get_text(strip=True) for th in thead.find_all('th')]
    if header:
      rows.append(header)
  tbody = tbl.find('tbody') or tbl
  for tr in tbody.find_all('tr'):
    row = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
    if row:
      rows.append(row)
  return rows

#Función para extraer las tablas, sus titulos y subtitulos procesar el contenido HTML
def processDataHTML(html):
  soup = bs(html, "html.parser")
  #Recupera la tabla que esta en el contenioHTML
  tables = soup.find_all('table')
  result = []
  for idx, table in enumerate(tables):
    title = ""
    caption = table.find('caption')
    if caption:
      title = caption.get_text(strip=True)
    else:
      prev = table.find_previous(['h2', 'h3', 'h4', 'h5', 'h6'])
      if prev:
        title = prev.get_text(strip=True)
    table_rows = processTableData(table)
    df = rowsToDataFrame(table_rows)
    result.append({'index': idx, 'title': title, 'dataframe': df})
  return result

#Función para mostrar las tablas encontradas y elegir que tabla visualizar
def mostrarTabla(tables_info):
  print(f"\nSe encontraron {len(tables_info)} tablas.")
  for t in tables_info:
    print(f"Tabla {t['index']}: {t['title'] or '(sin titulo)'}")
  try:
    idx = int(input("\nSeleccione el número de la tabla que desea visualizar: "))
    if 0 <= idx < len(tables_info):
      print(f"\nTabla {idx} - Título: {tables_info[idx]['title'] or '(sin titulo)'}")
      return tables_info[idx]['dataframe']
    else:
      print("Número de tabla inválido.")
  except ValueError:
    print("Entrada no válida. Debe ingresar un número entero.")

#Leer el WebSite
r = requests.get('https://en.wikipedia.org/wiki/List_of_Latin_phrases_(full)').text
#Formatea el contenido
tables_info = processDataHTML(r)
#Ejecuta la funcion para la recuperación de los datos de la tabla como dataframe de Pandas  se muestra
df = mostrarTabla(tables_info)
df

#Guardar datos en xlsx
df.to_excel("Tabla_seleccionada.xlsx")

import unicodedata
import re

tablax = pd.read_excel("Tabla_seleccionada.xlsx")
tablax

#Listar palabras palabras y verbos en Latin
pLatin = tablax['Notes']
pLatin

!pip install deep-translator
import requests
from deep_translator import GoogleTranslator
from collections import Counter

diccionario_url = "https://raw.githubusercontent.com/patofw/imf_master/master/proyecto_final_datos/Sentimientos.txt"
traduccionX = requests.get(diccionario_url).text

lineas = traduccionX.strip().splitlines()
palabras = []
valores = []

for linea in lineas:
    partes = linea.strip().split()
    if len(partes) >= 2:
        *palabra, valor = partes
        palabras.append(" ".join(palabra))
        valores.append(int(valor))

!pip install -U spacy
!python -m spacy download es_core_news_sm

import spacy

nlp = spacy.load("es_core_news_sm")

#Función para obtener el lema de la palabra del diccionario
def obtener_lemma(palabra):
  doc = nlp(palabra)
  return doc[0].lemma_ if len(doc) > 0 else palabra

#Usarlo en el diccionario traducido y crear un nuevo diccionario con los lemas
diccionario_url['lemma'] = diccionario_url['Palabra_es'].apply(obtener_lemma)

diccionario_lemma = dict(zip(diccionario_url['lemma'], diccionario_url['Valor']))
sentimiento_es = dict(zip(diccionario_url['Palabra_Lt'], diccionario_url['Valor']))

#Función para realizar encontrar las palabras con el lema

def analizar_sentimiento_lemma(texto):
    # Normalizar y extraer palabras
    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8')
    texto = texto.lower()

    doc = nlp(texto)

    encontradas = []
    puntaje_total =0

    for token in doc:
      if token.is_alpha:
        lema = token.lemma_
        if lema in diccionario_lemma:
          valor_dic = diccionario_lemma[lema]
          puntaje = valor_dic+ 1
          encontradas.append(f"{lema}: {puntaje}")
          puntaje_total += puntaje

    return pd.Series([", ".join(encontradas), puntaje_total])

tin

pLatin[['Encontradas_lema']] = pLatin['Notes'].apply(analizar_sentimiento_lemma)
pLatin.head(10)

#Función para contrar los verbos
def contar_verbos(texto):
  doc = nlp(texto)
  num_verbos = sum(1 for token in doc if token.pos_ == 'VERB')
  return num_verbos

#Agregar la columan del número de verbos
pLatin['Numero_verbos'] = pLatin['textDep'].apply(contar_verbos)
pLatin.head(10)